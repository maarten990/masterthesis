{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maarten/Documents/masterthesis/supervised\n"
     ]
    }
   ],
   "source": [
    "%cd supervised\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data\n",
    "import train\n",
    "import evaluate\n",
    "import models\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(num_clusters=15, window_size=5, old_test=False):\n",
    "    if window_size == 1:\n",
    "        window_label = 0\n",
    "    elif window_size % 2 == 0:\n",
    "        window_label = window_size // 2\n",
    "    else:\n",
    "        window_label = (window_size // 2) + 1\n",
    "\n",
    "    files = [f'../clustered_data/{num_clusters}/18{i:03d}.xml' for i in [1, 2, 3, 4, 5, 6, 210, 211]]\n",
    "    valid_files = [f'../clustered_data/{num_clusters}/18{i:03d}.xml' for i in [7, 209]]\n",
    "    test_files = [f'../clustered_data/{num_clusters}/{i}162.xml' for i in [14, 15, 16]]\n",
    "    all_files = files + valid_files + test_files\n",
    "\n",
    "    files_gmm = [f'../clustered_vgmm/{num_clusters}/18{i:03d}.xml' for i in [1, 2, 3, 4, 5, 6, 210, 211]]\n",
    "    valid_files_gmm = [f'../clustered_vgmm/{num_clusters}/18{i:03d}.xml' for i in [7, 209]]\n",
    "    test_files_gmm = [f'../clustered_vgmm/{num_clusters}/{i}162.xml' for i in [14, 15, 16]]\n",
    "    all_files_gmm = files_gmm + valid_files_gmm + test_files_gmm\n",
    "\n",
    "    vocab = data.GermanDataset(all_files, all_files_gmm, num_clusters, -1, window_size, window_label, char_tokens=True).vocab\n",
    "\n",
    "    if old_test:\n",
    "        dataset = data.GermanDataset(files, files_gmm, num_clusters, 1.0, window_size, window_label,\n",
    "                                     char_tokens=True,\n",
    "                                     vocab=vocab)\n",
    "        testset = data.GermanDataset(test_files, test_files_gmm, num_clusters, 1.0, window_size, window_label,\n",
    "                                     char_tokens=True,\n",
    "                                     vocab=vocab)\n",
    "    else:\n",
    "        dataset = data.GermanDataset(files + test_files, files_gmm + test_files_gmm, num_clusters, 1.0, window_size, window_label,\n",
    "                                     char_tokens=True,\n",
    "                                     vocab=vocab)\n",
    "    validset = data.GermanDataset(valid_files, valid_files_gmm, num_clusters, 1.0, window_size, window_label,\n",
    "                                  char_tokens=True,\n",
    "                                  vocab=vocab)\n",
    "    \n",
    "    if old_test:\n",
    "        return dataset, validset, testset\n",
    "    else:\n",
    "        return dataset, validset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main test\n",
    "\n",
    "window_size = 2\n",
    "num_clusters = 9\n",
    "dataset, validset = load_dataset(num_clusters, window_size)\n",
    "\n",
    "params = train.CNNParams(\n",
    "    embed_size=100,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (33, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "#             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "\n",
    "baseline = {}\n",
    "kmeans = {}\n",
    "gmm = {}\n",
    "\n",
    "for n in [50, 100, 400, 800, 1200, 1600, 2000, 2400]:\n",
    "    values = evaluate.cross_val(10, n, model_fns, [False, False], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=None)\n",
    "    baseline[n] = [v[0] for v in values]\n",
    "    kmeans[n] = [v[1] for v in values]\n",
    "#    gmm[n] = [v[2] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {s: {'Baseline': baseline[s],\n",
    "         'K-Means': kmeans[s],\n",
    "#         'Mixture model': gmm[s],\n",
    "        }\n",
    "    for s in baseline.keys()\n",
    "}\n",
    "evaluate.analyze_tseries(d, 'number of training samples', 'model', '../report/figures/results/main_window4_oldtest')\n",
    "evaluate.analyze_size(d, 'number of samples', 'model', '../report/figures/results/main_window4_oldtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate.analyze_wrapper(baseline, kmeans, None, 'model', '../report/figures/results/main_window4_data_oldtest')\n",
    "#evaluate.analyze_wrapper(baseline, kmeans, gmm, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on old data\n",
    "\n",
    "window_size = 4\n",
    "num_clusters = 9\n",
    "dataset, validset, testset = load_dataset(num_clusters, window_size, True)\n",
    "\n",
    "params = train.CNNParams(\n",
    "    embed_size=100,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (33, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "#             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "\n",
    "baseline = {}\n",
    "kmeans = {}\n",
    "gmm = {}\n",
    "\n",
    "for n in [50, 100, 400, 800, 1200, 1600, 2000, 2400]:\n",
    "    values = evaluate.cross_val(10, n, model_fns, [False, False], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=testset)\n",
    "    baseline[n] = [v[0] for v in values]\n",
    "    kmeans[n] = [v[1] for v in values]\n",
    "#    gmm[n] = [v[2] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train.CNNParams(\n",
    "    embed_size=100,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (33, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "    \n",
    "no_labels = {}\n",
    "with_labels = {}\n",
    "\n",
    "for window_size in 3, 5, 9, 7, 11, 15:\n",
    "    num_clusters = 9\n",
    "    dataset, validset = load_dataset(num_clusters, window_size)\n",
    "\n",
    "    values = evaluate.cross_val(10, 1200, model_fns, [False, False], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=None)\n",
    "    no_labels[window_size] = [v[0] for v in values]\n",
    "    with_labels[window_size] = [v[1] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dictionary:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Creating dictionary:  15%|█▌        | 2/13 [00:00<00:00, 13.81it/s]\u001b[A\n",
      "Creating dictionary:  23%|██▎       | 3/13 [00:00<00:01,  6.96it/s]\u001b[A\n",
      "Creating dictionary:  31%|███       | 4/13 [00:00<00:01,  7.44it/s]\u001b[A\n",
      "Creating dictionary:  46%|████▌     | 6/13 [00:00<00:01,  6.98it/s]\u001b[A\n",
      "Creating dictionary:  54%|█████▍    | 7/13 [00:01<00:00,  6.59it/s]Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "Creating dictionary: 100%|██████████| 13/13 [00:02<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1254 positive samples, 58709 negative samples.\n",
      "Retrieved 562 positive samples, 21031 negative samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 691 positive samples, 37708 negative samples.\n",
      "1600 training samples, 1124 testing samples\n",
      "[138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[256, 83, 7], so expected input[50, 138, 83] to have 83 channels, but got 138 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8b434d5f6b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                 \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                 testset=testset)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mno_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mwith_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masterthesis/supervised/evaluate.py\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(k, train_size, model_fns, use_dist_list, optim_fn, dataset, params, early_stopping, validation_set, testset, gpu)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0muse_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             )\n\u001b[1;32m    240\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masterthesis/supervised/train.py\u001b[0m in \u001b[0;36msetup_and_train\u001b[0;34m(params, model_fn, optim_fn, dataset, epochs, batch_size, gpu, early_stopping, progbar, max_norm, validation_set, use_dist)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0muse_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masterthesis/supervised/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, epochs, gpu, early_stopping, progbar, max_norm, validation_set, use_dist)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masterthesis/supervised/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masterthesis/supervised/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0monehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight[256, 83, 7], so expected input[50, 138, 83] to have 83 channels, but got 138 channels instead"
     ]
    }
   ],
   "source": [
    "params = train.CharCNNParams(\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "             lambda r: models.ClusterLabelsChar(r, num_clusters, window_size, params.dropout)]\n",
    "\n",
    "no_labels = {}\n",
    "with_labels = {}\n",
    "for window_size in [3, 4, 5]:\n",
    "    num_clusters = 9\n",
    "    dataset, validset, testset = load_dataset(num_clusters, window_size, True)\n",
    "\n",
    "    values = evaluate.cross_val(10, 1600, model_fns, [False, False], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=testset)\n",
    "    no_labels[window_size] = [v[0] for v in values]\n",
    "    with_labels[window_size] = [v[1] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.GermanDataset at 0x7fb6983bf8d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 training samples, 1124 testing samples\n",
      "[138]\n",
      "[138]\n"
     ]
    }
   ],
   "source": [
    "values = evaluate.cross_val(10, 1600, model_fns, [False, False], optim_fn, dataset, params,\n",
    "                            early_stopping=10,\n",
    "                            validation_set=validset,\n",
    "                            testset=testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {s: {'Baseline': no_labels[s],\n",
    "         'K-Means': with_labels[s],\n",
    "        }\n",
    "    for s in no_labels.keys()\n",
    "}\n",
    "#evaluate.analyze_tseries(d, 'window size', 'model')\n",
    "evaluate.analyze_tseries(d, 'window size', 'model', '../report/figures/results/800-windowsize_oldtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train.CNNParams(\n",
    "    embed_size=100,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (33, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "#             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "    \n",
    "kmeans = {}\n",
    "gmm = {}\n",
    "\n",
    "for num_clusters in [2, 3, 5, 7, 9, 15, 30]:\n",
    "    window_size = 5\n",
    "    dataset, validset = load_dataset(num_clusters, window_size)\n",
    "\n",
    "    values = evaluate.cross_val(10, 1200, model_fns, [False], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=None)\n",
    "    kmeans[num_clusters] = [v[0] for v in values]\n",
    "    gmm[num_clusters] = [v[1] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {s: {'K-Means': kmeans[s],\n",
    "         #'Mixture model': gmm[s],\n",
    "        }\n",
    "    for s in kmeans.keys()\n",
    "}\n",
    "#evaluate.analyze_tseries(d, 'k', 'model')\n",
    "evaluate.analyze_size(d, 'k', 'model', '../report/figures/results/800-numcluster')\n",
    "evaluate.analyze_tseries(d, 'k', 'model', '../report/figures/results/800-numcluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test num clusters on old set\n",
    "\n",
    "params = train.CNNParams(\n",
    "    embed_size=100,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (33, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim_fn = lambda p: torch.optim.Adadelta(p)\n",
    "model_fns = [lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout),\n",
    "             lambda r: models.CategoricalClusterLabels(r, num_clusters, window_size, params.dropout)]\n",
    "    \n",
    "kmeans = {}\n",
    "gmm = {}\n",
    "\n",
    "for num_clusters in [2, 3, 5, 7, 9, 15, 30]:\n",
    "    window_size = 4\n",
    "    dataset, validset, testset = load_dataset(num_clusters, window_size, True)\n",
    "\n",
    "    values = evaluate.cross_val(10, 1600, model_fns, [False, True], optim_fn, dataset, params,\n",
    "                                early_stopping=10,\n",
    "                                validation_set=validset,\n",
    "                                testset=testset)\n",
    "    kmeans[num_clusters] = [v[0] for v in values]\n",
    "    gmm[num_clusters] = [v[1] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {s: {'K-Means': kmeans[s],\n",
    "         'Mixture model': gmm[s],\n",
    "        }\n",
    "    for s in kmeans.keys()\n",
    "}\n",
    "evaluate.analyze_tseries(d, 'k', 'model')\n",
    "#evaluate.analyze_size(d, 'k', 'model', '../report/figures/results/800-numcluster_olddata')\n",
    "#evaluate.analyze_tseries(d, 'k', 'model', '../report/figures/results/800-numcluster_olddata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test regularization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = train.CNNParams(\n",
    "    embed_size=300,\n",
    "    dropout=0.0,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (34, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=0,\n",
    ")\n",
    "\n",
    "optim_no_decay = lambda p: torch.optim.Adadelta(p)\n",
    "\n",
    "dropout_tests = []\n",
    "dropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for value in dropout_values:\n",
    "    params.dropout = value\n",
    "    dropout_tests.append(evaluate.cross_val(10, -1,\n",
    "                                            lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "                                            optim_no_decay, only_idx_dataset, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.analyze(dict(zip(dropout_values, dropout_tests)), 'dropout rate', '../report/figures/results/dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train.CNNParams(\n",
    "    embed_size=300,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (34, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=3,\n",
    ")\n",
    "\n",
    "optim = lambda p: torch.optim.Adadelta(p)\n",
    "\n",
    "l2norm_tests = []\n",
    "l2norm_values = [1, 3, 4, 5, 10, 0]\n",
    "for value in l2norm_values:\n",
    "    params.dropout = 0.5\n",
    "    params.max_norm = value\n",
    "    l2norm_tests.append(evaluate.cross_val(10, -1,\n",
    "                                           lambda r: models.NoClusterLabels(r, params.dropout),\n",
    "                                           optim, only_idx_dataset, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate.analyze(dict(zip(map(lambda v: v if v != 0 else 'None', l2norm_values), l2norm_tests)),\n",
    "                 'maximum L2 norm of weight vectors', '../report/figures/results/decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train.CNNParams(\n",
    "    embed_size=300,\n",
    "    dropout=0.5,\n",
    "    epochs=100,\n",
    "    filters=[(33, 3), (34, 5), (33, 7)],\n",
    "    num_layers=1,\n",
    "    max_norm=2,\n",
    ")\n",
    "\n",
    "bn_tests = []\n",
    "optim_fn = lambda p: torch.optim.SGD(p, lr=0.01, momentum=0.9, nesterov=True)\n",
    "bn_tests.append(evaluate.cross_val(10, 400,\n",
    "                                   lambda r: models.NoClusterLabels(r, params.dropout, batch_norm=False),\n",
    "                                   optim_fn, only_idx_dataset, params))\n",
    "bn_tests.append(evaluate.cross_val(10, 400,\n",
    "                                   lambda r: models.NoClusterLabels(r, params.dropout, batch_norm=True),\n",
    "                                   optim_fn, only_idx_dataset, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.analyze({'no batchnorm': bn_tests[0],\n",
    "                  'batchnorm': bn_tests[1]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
